name: 'ppo_minigrid_doorkey_6x6_lstm'

env:
  name: vel_miniworld.env.minigrid
  envname: 'MiniGrid-DoorKey-6x6-v0'


vec_env:
  name: vel.rl.vecenv.subproc


model:
  name: vel.rl.models.policy_gradient_rnn_model

  backbone:
    name: vel_miniworld.model.minigrid_obs_lstm
    hidden_layers: [256]
    lstm_dim: 128
    activation: 'tanh'
    normalization: 'layer'


reinforcer:
  name: vel.rl.reinforcers.on_policy_iteration_reinforcer

  algo:
    name: vel.rl.algo.policy_gradient.ppo
#    normalize_advantage: off

    entropy_coefficient: 0.01
    value_coefficient: 0.5

    max_grad_norm: 0.5 # Gradient clipping parameter

    cliprange:
      name: vel.schedules.linear
      initial_value: 0.1
      final_value: 0.0

  env_roller:
    name: vel.rl.env_roller.vec.step_env_roller
    gae_lambda: 0.95 # Generalized Advantage Estimator Lambda parameter
    number_of_steps: 128 # How many environment steps go into a single batch

  parallel_envs: 8 # How many environments to run in parallel
  batch_size: 256 # How many samples can go into the model once
  experience_replay: 4 # How many times to replay the experience

  discount_factor: 0.99 # Discount factor for the rewards

  shuffle_transitions: off  # Required for RNN policies


optimizer:
  name: vel.optimizers.adam
  lr: 2.5e-4
  epsilon: 1.0e-5


scheduler:
  name: vel.scheduler.linear_batch_scaler


commands:
  train:
    name: vel.rl.commands.rl_train_command
    total_frames: 1.0e6
    batches_per_epoch: 10

  record:
    name: vel.rl.commands.record_movie_command
    takes: 10
    videoname: 'ppo_minigrid_doorkey_6x6_lstm_vid_{:04}.avi'
#    frame_history: 4
    sample_args:
      argmax_sampling: true
